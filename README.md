# Text Summarizer with Fine-Tuned Pegasus Model

## Overview

This project demonstrates how to fine-tune a pre-trained model (`google/pegasus-cnn_dailymail`) for the task of **English text summarization**. The model was fine-tuned using the **Samsum dataset**, which contains over 16K conversations between two people, resembling chat-like dialogues. The aim of the project is to generate summaries that are more related to the specific dialogues from the dataset, enhancing the performance of the summarization model on dialogue-based content.

A copy of the trained model and tokenizer can be found on [Google Drive](https://drive.google.com/drive/folders/1-V95H3E-KAgCD3-6_EooW8PJooYLvC6P?usp=drive_link).


## Project Details

### Libraries and Tools Used:
- **Hugging Face Transformers**: Hugging Face was chosen for this project as it provides one of the best ecosystems for working with NLP models, especially when it comes to fine-tuning pre-trained models like Pegasus.
- **PyTorch**: The model was fine-tuned using the PyTorch framework, which is widely used in the NLP community for training and deploying deep learning models.
- **Google Colab**: The model was trained in the cloud using Google Colab, as training NLP models requires substantial computational resources (e.g., GPU/TPU support).

### Dataset:
The dataset used is the **Samsum Dataset**, which consists of over 16,000 conversation pairs that simulate dialogues between two people. These dialogues are structured in a way that they could be part of a real conversation, making it ideal for training a summarization model for dialogue-based content.

### Pre-trained Model:
- **Model Used**: `google/pegasus-cnn_dailymail`
- This pre-trained model is specifically designed for summarization tasks and was originally fine-tuned on news articles. Since the Samsum dataset is quite different (conversations vs. news), fine-tuning is required to adapt the model for better performance on this new task.

### Fine-Tuning the Model:
- **Fine-Tuning Process**: The pre-trained Pegasus model was fine-tuned on the Samsum dataset using a small number of epochs. This process is important because the original Pegasus model was not trained on the Samsum dataset, and fine-tuning helps the model adapt to summarize dialogue-based content.
- **Results**: As the model was trained with only a few epochs, the generated summaries may not exactly match the summaries in the original Samsum dataset. However, the summaries generated by the fine-tuned model are relevant to the dialogues provided, improving the overall performance on this new task.

### Cloud Training:
Due to the large size of the model and the dataset, training was conducted in a cloud environment (Google Colab) using available GPUs, allowing for faster training and better resource utilization.

## Files

- **`pegasus-samsum-model`**: The directory containing the fine-tuned model and its weights.
- **`tokenizer`**: The tokenizer used for converting the input text into the format required by the model and decoding the model outputs back into human-readable text.
- **`TextSummarizer`**: Folder containing the saved model and tokenizer files.

## How to Use the Model

1. Clone this repository or download the files.
2. Make sure you have the required libraries installed:
    ```bash
    pip install transformers torch datasets
    ```
3. Load the fine-tuned model and tokenizer:

    ```python
    from transformers import pipeline, AutoTokenizer
    
    # Load the tokenizer
    tokenizer = AutoTokenizer.from_pretrained("pegasus-samsum-model")
    
    # Load the fine-tuned model
    model = pipeline("summarization", model="pegasus-samsum-model", tokenizer=tokenizer)
    ```

4. To generate a summary for any dialogue, pass the dialogue to the model:
    
    ```python
    dialogue = "Your conversation text goes here."
    summary = model(dialogue)
    print(summary)
    ```

## How to Fine-Tune the Model

If you want to fine-tune the model with your own dataset, follow these steps:

1. Prepare your dataset in the required format.
2. Tokenize the dataset using the tokenizer.
3. Set up the **TrainingArguments** and **Trainer** in Hugging Face’s `transformers` library.
4. Start the training process.

You can refer to the code in this repository for specific instructions on fine-tuning the model.

## Limitations

- The model was fine-tuned with only a few epochs. As a result, the summaries might not always be highly accurate or close to the original summaries in the Samsum dataset. However, the fine-tuned model generates summaries relevant to the input dialogues.
- As the model was fine-tuned for dialogue-based content, it may not perform as well on other types of text (e.g., news articles).

## Conclusion

This project demonstrates the process of fine-tuning a pre-trained model (Pegasus) for dialogue-based text summarization using the Samsum dataset. The fine-tuned model can generate meaningful summaries based on conversations, making it useful for tasks such as summarizing dialogues in chat applications or conversational datasets.

## Future Improvements

- **More Epochs**: Fine-tuning the model for more epochs could improve the quality of the summaries.
- **Larger Dataset**: Using a larger dataset or augmenting the existing one might also improve the model’s performance.
- **Model Evaluation**: The model's performance can be evaluated using different metrics such as ROUGE scores.

